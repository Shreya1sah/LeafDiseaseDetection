{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daeff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define class names\n",
    "CLASS_NAMES = [\n",
    "    'Apple_Scab','Black_Rot','Cedar_Apple_Rust','Healthy','not_leaf'\n",
    "]\n",
    "\n",
    "# Data Augmentation\n",
    "def augment(path, IMG_DIM):\n",
    "    datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                 shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "    for class_name in CLASS_NAMES:\n",
    "        curPath = os.path.join(path, class_name)\n",
    "        if not os.path.isdir(curPath):\n",
    "            continue\n",
    "\n",
    "        images = glob.glob(curPath + '/*')\n",
    "        temp = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in images]\n",
    "        temp = [img.reshape((1,) + img.shape) for img in temp]\n",
    "\n",
    "        for batch in datagen.flow(temp, batch_size=4, save_to_dir=curPath, save_format='jpg'):\n",
    "            if len(images) + len(batch) * 4 > 800:\n",
    "                break\n",
    "\n",
    "# Creating Frame\n",
    "def createFrame(path, IMG_DIM):\n",
    "    train_imgs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(CLASS_NAMES):\n",
    "        curPath = os.path.join(path, class_name)\n",
    "        if not os.path.isdir(curPath):\n",
    "            continue\n",
    "\n",
    "        images = glob.glob(curPath + '/*')\n",
    "        temp = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in images]\n",
    "\n",
    "        train_imgs.extend(temp)\n",
    "        labels.extend([idx] * len(images))\n",
    "\n",
    "    df = pd.DataFrame(list(zip(train_imgs, labels)))\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Stratified K-Fold Split\n",
    "def kFold(df):\n",
    "    df['kfold'] = -1\n",
    "    df = df.reset_index(drop=True)\n",
    "    y = df[1]\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    for f, (_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    return df\n",
    "\n",
    "# Customized CNN models\n",
    "def DenseNet(train_imgs, train_labels, num_classes, num_epochs=20):\n",
    "    print(\"-------------------------------------DENSENET--------------------------------------------\")\n",
    "    input_shape = (128, 128, 3)\n",
    "    base_model = keras.applications.DenseNet169(include_top=False, weights=\"imagenet\", input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_imgs, train_labels, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "    return model\n",
    "\n",
    "def Inception(train_imgs, train_labels, num_classes, num_epochs=20):\n",
    "    print(\"-------------------------------------INCEPTION-------------------------------------------\")\n",
    "\n",
    "    base_model = keras.applications.InceptionV3(input_shape=(128, 128, 3), include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(1028, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_imgs, train_labels, epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    return model\n",
    "\n",
    "def Xception(train_imgs, train_labels, num_classes, num_epochs=20):\n",
    "    print(\"-------------------------------------XCEPTION---------------------------------------------\")\n",
    "    \n",
    "    base_model = keras.applications.Xception(input_shape=(128, 128, 3), include_top=False, weights=\"imagenet\")\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_imgs, train_labels, epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = createFrame(\"dataset/Train\", (128, 128, 3))\n",
    "df = kFold(df)\n",
    "\n",
    "print(df.head())  # Prints first 5 rows of the dataset DataFrame\n",
    "print(df['kfold'].value_counts())  # Prints count of images per fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following will display the first image in your dataset.\n",
    "#To double-check the dataset before training,we need to run this:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert first image array to numpy array\n",
    "img_array = np.array(df.iloc[0, 0])\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img_array.astype('uint8'))  # Convert float values to uint8 for display\n",
    "plt.title(f\"Class Label: {df.iloc[0, 1]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert second image array to numpy array\n",
    "img_array = np.array(df.iloc[1, 0])  # Change index from 0 to 1\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img_array.astype('uint8'))  # Convert float values to uint8 for display\n",
    "plt.title(f\"Class Label: {df.iloc[1, 1]}\")  # Update index for the label as well\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16912ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generateRank1(score, class_no):\n",
    "    return 1 - np.exp(-((score - 1) ** 2) / 2.0)\n",
    "\n",
    "def generateRank2(score, class_no):\n",
    "    return 1 - np.tanh(((score - 1) ** 2) / 2)\n",
    "\n",
    "def doFusion(res1, res2, res3, labels, class_no):\n",
    "    correct = 0\n",
    "    predicted_classes = []\n",
    "\n",
    "    for i in range(len(res1)):\n",
    "        rank1, rank2, rank3 = (\n",
    "            generateRank1(res1[i], class_no) * generateRank2(res1[i], class_no),\n",
    "            generateRank1(res2[i], class_no) * generateRank2(res2[i], class_no),\n",
    "            generateRank1(res3[i], class_no) * generateRank2(res3[i], class_no),\n",
    "        )\n",
    "\n",
    "        rankSum = rank1 + rank2 + rank3\n",
    "        predicted_class = np.argmin(rankSum)\n",
    "\n",
    "        if predicted_class < class_no and labels[i][predicted_class] == 1:\n",
    "            correct += 1\n",
    "        predicted_classes.append(predicted_class)\n",
    "\n",
    "    accuracy = correct / len(res1)\n",
    "    print(f\" Ensemble Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return predicted_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define class names\n",
    "CLASS_NAMES = [\n",
    "    'Apple_Scab','Black_Rot','Cedar_Apple_Rust','Healthy','not_leaf'\n",
    "]\n",
    "\n",
    "#  Manually define dataset path & number of epochs (no argparse issues)\n",
    "path1 = 'dataset/Train'  # Update if needed\n",
    "num_epochs = 20  # Change this value if needed\n",
    "\n",
    "#  Set image dimensions & number of classes\n",
    "IMG_DIM = (128, 128, 3)\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "#  Load dataset and apply k-fold splitting\n",
    "df = createFrame(path1, IMG_DIM)\n",
    "df = kFold(df)\n",
    "\n",
    "#  Training loop for k-fold cross-validation\n",
    "for i in range(1, 6):  # 5-fold cross-validation\n",
    "    print(f\"-------------------- FOLD {i} --------------------\")\n",
    "\n",
    "    dfTrain = df[df['kfold'] != i]\n",
    "    dfTest = df[df['kfold'] == i]\n",
    "\n",
    "    print(f\"Fold {i}: Train size = {len(dfTrain)}, Test size = {len(dfTest)}\")\n",
    "\n",
    "    #  Skip fold if test set is empty\n",
    "    if len(dfTest) == 0:\n",
    "        print(f\" Skipping Fold {i} (No test samples available)\")\n",
    "        continue\n",
    "\n",
    "    # Convert images to NumPy arrays and normalize\n",
    "    train_imgs = np.array(list(dfTrain[0])) / 255.0\n",
    "    train_labels = to_categorical(LabelEncoder().fit_transform(dfTrain[1]))\n",
    "\n",
    "    test_imgs = np.array(list(dfTest[0])) / 255.0\n",
    "    test_labels = to_categorical(LabelEncoder().fit_transform(dfTest[1]))\n",
    "\n",
    "    # Train three CNN models\n",
    "    print(\" Training DenseNet...\")\n",
    "    model0 = DenseNet(train_imgs, train_labels, num_classes=num_classes, num_epochs=num_epochs)\n",
    "\n",
    "    print(\" Training Inception...\")\n",
    "    model1 = Inception(train_imgs, train_labels, num_classes=num_classes, num_epochs=num_epochs)\n",
    "\n",
    "    print(\" Training Xception...\")\n",
    "    model2 = Xception(train_imgs, train_labels, num_classes=num_classes, num_epochs=num_epochs)\n",
    "\n",
    "    print(\" Model Evaluation\")\n",
    "    model0.evaluate(test_imgs, test_labels, batch_size=32)\n",
    "    model1.evaluate(test_imgs, test_labels, batch_size=32)\n",
    "    model2.evaluate(test_imgs, test_labels, batch_size=32)\n",
    "\n",
    "    # Get model predictions\n",
    "    res1, res2, res3 = model1.predict(test_imgs), model0.predict(test_imgs), model2.predict(test_imgs)\n",
    "    \n",
    "    # Perform ensemble learning (fusion)\n",
    "    predictedClass = doFusion(res1, res2, res3, test_labels, class_no=num_classes)\n",
    "\n",
    "    print(\" **DenseNet Evaluation**\")\n",
    "    print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res1, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    print(\" **Inception Evaluation**\")\n",
    "    print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res2, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    print(\" **Xception Evaluation**\")\n",
    "    print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res3, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "    print(\" **Ensembled Results**\")\n",
    "    print(classification_report(np.argmax(test_labels, axis=-1), predictedClass, target_names=CLASS_NAMES, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models after training in Code 3\n",
    "model0.save(\"densenet_model.h5\")\n",
    "model1.save(\"inception_model.h5\")\n",
    "model2.save(\"xception_model.h5\")\n",
    "print(\"Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())  # This prints all files in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92510c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical  #  Fixed Import\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load trained models\n",
    "print(\"Loading trained models...\")\n",
    "model_densenet = keras.models.load_model(\"densenet_model.h5\")\n",
    "model_inception = keras.models.load_model(\"inception_model.h5\")\n",
    "model_xception = keras.models.load_model(\"xception_model.h5\")\n",
    "\n",
    "#  Debugging Step 1: Verify Model Structure\n",
    "print(\"Checking model structures...\")\n",
    "model_densenet.summary()\n",
    "model_inception.summary()\n",
    "model_xception.summary()\n",
    "\n",
    "# Load test dataset\n",
    "test_path = \"dataset/Test\"\n",
    "\n",
    "# Function to create DataFrame from test images\n",
    "def load_test_data(path, IMG_DIM):\n",
    "    test_imgs = []\n",
    "    labels = []\n",
    "    directories = os.listdir(path)\n",
    "\n",
    "    class_names = ['Apple_Scab','Black_Rot','Cedar_Apple_Rust','Healthy','not_leaf']\n",
    "\n",
    "    for directory in directories:\n",
    "        curPath = os.path.join(path, directory)\n",
    "        if not os.path.isdir(curPath):\n",
    "            continue\n",
    "\n",
    "        images = glob.glob(curPath + '/*')\n",
    "        temp = [keras.preprocessing.image.img_to_array(\n",
    "            keras.preprocessing.image.load_img(img, target_size=IMG_DIM)) for img in images]\n",
    "\n",
    "        test_imgs.extend(temp)\n",
    "        labels.extend([class_names.index(directory)] * len(images))\n",
    "\n",
    "    df = pd.DataFrame(list(zip(test_imgs, labels)))\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Load test dataset\n",
    "IMG_DIM = (128, 128, 3)\n",
    "df_test = load_test_data(test_path, IMG_DIM)\n",
    "\n",
    "# Normalize images\n",
    "test_imgs = np.array(list(df_test[0])) / 255.0\n",
    "test_labels = np.array(df_test[1])\n",
    "\n",
    "# Convert labels to categorical\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_labels)\n",
    "test_labels = encoder.transform(test_labels)\n",
    "test_labels = to_categorical(test_labels)  #  Fixed Here\n",
    "\n",
    "#  Debugging Step 2: Check Test Data Properties\n",
    "print(f\"Test dataset shape: {test_imgs.shape}\")\n",
    "print(f\"Sample test image min/max values: {test_imgs.min()}, {test_imgs.max()}\")\n",
    "print(f\"Unique class labels in test set: {np.unique(df_test[1])}\")\n",
    "\n",
    "# Predict using each model\n",
    "res_densenet = model_densenet.predict(test_imgs)\n",
    "res_inception = model_inception.predict(test_imgs)\n",
    "res_xception = model_xception.predict(test_imgs)\n",
    "\n",
    "#  Debugging Step 3: Print Some Predictions\n",
    "print(\"First 5 DenseNet predictions:\", np.argmax(res_densenet[:5], axis=-1))\n",
    "print(\"First 5 Inception predictions:\", np.argmax(res_inception[:5], axis=-1))\n",
    "print(\"First 5 Xception predictions:\", np.argmax(res_xception[:5], axis=-1))\n",
    "print(\"Actual first 5 labels:\", np.argmax(test_labels[:5], axis=-1))\n",
    "\n",
    "# Fuzzy Rank-Based Fusion Function\n",
    "def generateRank1(score, class_no):\n",
    "    rank = np.zeros([class_no, 1])\n",
    "    for i in range(class_no):\n",
    "        rank[i] = 1 - np.exp(-((score[i] - 1) ** 2) / 2.0)\n",
    "    return rank\n",
    "\n",
    "def generateRank2(score, class_no):\n",
    "    rank = np.zeros([class_no, 1])\n",
    "    for i in range(class_no):\n",
    "        rank[i] = 1 - np.tanh(((score[i] - 1) ** 2) / 2)\n",
    "    return rank\n",
    "\n",
    "def doFusion(res1, res2, res3, labels, class_no):\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    for i in range(len(res1)):\n",
    "        rank1 = generateRank1(res1[i], class_no) * generateRank2(res1[i], class_no)\n",
    "        rank2 = generateRank1(res2[i], class_no) * generateRank2(res2[i], class_no)\n",
    "        rank3 = generateRank1(res3[i], class_no) * generateRank2(res3[i], class_no)\n",
    "\n",
    "        rankSum = rank1 + rank2 + rank3\n",
    "        scoreSum = 1 - (res1[i] + res2[i] + res3[i]) / 3\n",
    "\n",
    "        fusedScore = (rankSum.T) * scoreSum\n",
    "        predicted_class = np.argmin(rankSum)\n",
    "\n",
    "        if predicted_class < class_no and labels[i][predicted_class] == 1:\n",
    "            correct += 1\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "    accuracy = correct / len(res1)\n",
    "    print(f\"\\nEnsemble Accuracy: {accuracy:.4f}\")\n",
    "    return predictions\n",
    "\n",
    "# Perform ensemble classification\n",
    "predicted_classes = doFusion(res_densenet, res_inception, res_xception, test_labels, class_no=5)\n",
    "\n",
    "# Classification Reports\n",
    "CLASS_NAMES = ['Apple_Scab','Black_Rot','Cedar_Apple_Rust','Healthy','not_leaf']\n",
    "\n",
    "print(\"\\n **DenseNet Evaluation**\")\n",
    "print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res_densenet, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "print(\"\\n **Inception Evaluation**\")\n",
    "print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res_inception, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "print(\"\\n **Xception Evaluation**\")\n",
    "print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res_xception, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "print(\"\\n **Ensembled Results**\")\n",
    "print(classification_report(np.argmax(test_labels, axis=-1), predicted_classes, target_names=CLASS_NAMES, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "#  Load trained models\n",
    "print(\"Loading trained models...\")\n",
    "model_densenet = keras.models.load_model(\"densenet_model.h5\")\n",
    "model_inception = keras.models.load_model(\"inception_model.h5\")\n",
    "model_xception = keras.models.load_model(\"xception_model.h5\")\n",
    "\n",
    "#  Define class labels\n",
    "CLASS_NAMES = [\"Apple_Scab\",\"Black_Rot\",\"Cedar_Apple_Rust\",\"Healthy\",\"not_leaf\"]\n",
    "\n",
    "#  Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    img = load_img(image_path, target_size=target_size)  # Load image\n",
    "    img_array = img_to_array(img)  # Convert to array\n",
    "    img_array = img_array / 255.0  # Normalize (0-1 scaling)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img, img_array\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_image(image_path):\n",
    "    img, img_array = preprocess_image(image_path)\n",
    "    \n",
    "    #  Model Predictions\n",
    "    pred_densenet = np.argmax(model_densenet.predict(img_array), axis=-1)[0]\n",
    "    pred_inception = np.argmax(model_inception.predict(img_array), axis=-1)[0]\n",
    "    pred_xception = np.argmax(model_xception.predict(img_array), axis=-1)[0]\n",
    "\n",
    "    #  Ensemble Prediction (Majority Vote)\n",
    "    all_preds = [pred_densenet, pred_inception, pred_xception]\n",
    "    ensemble_pred = max(set(all_preds), key=all_preds.count)  # Majority class\n",
    "\n",
    "    #  Display results\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predictions:\\nDenseNet: {CLASS_NAMES[pred_densenet]}\\n\"\n",
    "              f\"Inception: {CLASS_NAMES[pred_inception]}\\n\"\n",
    "              f\"Xception: {CLASS_NAMES[pred_xception]}\\n\"\n",
    "              f\"Ensemble (Final): {CLASS_NAMES[ensemble_pred]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n Prediction Results:\")\n",
    "    print(f\"DenseNet: {CLASS_NAMES[pred_densenet]}\")\n",
    "    print(f\"Inception: {CLASS_NAMES[pred_inception]}\")\n",
    "    print(f\"Xception: {CLASS_NAMES[pred_xception]}\")\n",
    "    print(f\" Ensemble Final Prediction: {CLASS_NAMES[ensemble_pred]}\")\n",
    "\n",
    "#  Test the function with an image\n",
    "image_path = \"apple_scab.jpg\"  \n",
    "predict_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afff068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # Import at the top\n",
    "\n",
    "# Train models and save history\n",
    "history_densenet = model0.fit(train_imgs, train_labels, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "history_inception = model1.fit(train_imgs, train_labels, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "history_xception = model2.fit(train_imgs, train_labels, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "\n",
    "#  Save Training History\n",
    "with open(\"history_densenet.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history_densenet.history, f)\n",
    "with open(\"history_inception.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history_inception.history, f)\n",
    "with open(\"history_xception.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history_xception.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train Data into Training & Validation (80-20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "    train_imgs, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Train models with validation data\n",
    "history_densenet = model0.fit(\n",
    "    train_imgs, train_labels, \n",
    "    batch_size=32, epochs=num_epochs, verbose=1,\n",
    "    validation_data=(val_imgs, val_labels)\n",
    ")\n",
    "\n",
    "history_inception = model1.fit(\n",
    "    train_imgs, train_labels, \n",
    "    batch_size=32, epochs=num_epochs, verbose=1,\n",
    "    validation_data=(val_imgs, val_labels)\n",
    ")\n",
    "\n",
    "history_xception = model2.fit(\n",
    "    train_imgs, train_labels, \n",
    "    batch_size=32, epochs=num_epochs, verbose=1,\n",
    "    validation_data=(val_imgs, val_labels)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plots training accuracy & loss for a given model history.\"\"\"\n",
    "    if history is None:\n",
    "        print(f\"[WARNING] No history data found for {model_name}!\")\n",
    "        return\n",
    "\n",
    "    #  Print available history keys for debugging\n",
    "    print(f\"Keys in {model_name} history:\", history.history.keys())\n",
    "\n",
    "    #  Ensure correct key names for Keras versions\n",
    "    acc_key = \"accuracy\" if \"accuracy\" in history.history else \"acc\"\n",
    "    val_acc_key = \"val_accuracy\" if \"val_accuracy\" in history.history else \"val_acc\"\n",
    "    loss_key = \"loss\"\n",
    "    val_loss_key = \"val_loss\"\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    #  Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[acc_key], label=\"Training Accuracy\", color='blue', linestyle='dashed', marker='o')\n",
    "    plt.plot(history.history[val_acc_key], label=\"Validation Accuracy\", color='green', linestyle='solid', marker='s')\n",
    "    plt.title(f\"{model_name} Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    #  Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[loss_key], label=\"Training Loss\", color='red', linestyle='dashed', marker='o')\n",
    "    plt.plot(history.history[val_loss_key], label=\"Validation Loss\", color='orange', linestyle='solid', marker='s')\n",
    "    plt.title(f\"{model_name} Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()  # Ensure plots are displayed\n",
    "\n",
    "\n",
    "#  Call this function after training each model\n",
    "plot_training_history(history_densenet, \"DenseNet\")\n",
    "plot_training_history(history_inception, \"Inception\")\n",
    "plot_training_history(history_xception, \"Xception\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert and save DenseNet model\n",
    "model = tf.keras.models.load_model(\"densenet_model.h5\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"densenet_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Convert and save Inception model\n",
    "model = tf.keras.models.load_model(\"inception_model.h5\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"inception_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Convert and save Xception model\n",
    "model = tf.keras.models.load_model(\"xception_model.h5\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(\"xception_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b392454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG19(train_imgs, train_labels, num_classes, num_epochs=20):\n",
    "    print(\"-------------------------------------VGG19---------------------------------------------\")\n",
    "    \n",
    "    base_model = keras.applications.VGG19(include_top=False, weights=\"imagenet\", input_shape=(128, 128, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_imgs, train_labels, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet18(train_imgs, train_labels, num_classes, num_epochs=20):\n",
    "    print(\"-------------------------------------RESNET18------------------------------------------\")\n",
    "\n",
    "    # Load ResNet50 and simulate ResNet18 by freezing deeper layers (ResNet18 isn't in Keras, so ResNet50 is a proxy)\n",
    "    base_model = keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(base_model.input, x)\n",
    "    model.compile(optimizer=RMSprop(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    model.fit(train_imgs, train_labels, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "    return model\n",
    "print(\" Training VGG19...\")\n",
    "model_vgg19 = VGG19(train_imgs, train_labels, num_classes=num_classes, num_epochs=num_epochs)\n",
    "\n",
    "print(\" Training ResNet18...\")\n",
    "model_resnet18 = ResNet18(train_imgs, train_labels, num_classes=num_classes, num_epochs=num_epochs)\n",
    "\n",
    "print(\" Evaluating VGG19 and ResNet18\")\n",
    "model_vgg19.evaluate(test_imgs, test_labels, batch_size=32)\n",
    "model_resnet18.evaluate(test_imgs, test_labels, batch_size=32)\n",
    "\n",
    "res_vgg19 = model_vgg19.predict(test_imgs)\n",
    "res_resnet18 = model_resnet18.predict(test_imgs)\n",
    "model_vgg19.save(\"vgg19_model.h5\")\n",
    "model_resnet18.save(\"resnet18_model.h5\")\n",
    "print(\" **VGG19 Evaluation**\")\n",
    "print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res_vgg19, axis=-1), target_names=CLASS_NAMES, digits=4))\n",
    "\n",
    "print(\" **ResNet18 Evaluation**\")\n",
    "print(classification_report(np.argmax(test_labels, axis=-1), np.argmax(res_resnet18, axis=-1), target_names=CLASS_NAMES, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a551284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get predictions from base models on the full test set\n",
    "res_densenet = model_densenet.predict(test_imgs)\n",
    "res_inception = model_inception.predict(test_imgs)\n",
    "res_xception = model_xception.predict(test_imgs)\n",
    "\n",
    "# Stack class predictions from all models\n",
    "pred_stack = np.stack([\n",
    "    np.argmax(res_densenet, axis=1),\n",
    "    np.argmax(res_inception, axis=1),\n",
    "    np.argmax(res_xception, axis=1)\n",
    "], axis=0)  # shape: (3, 1887)\n",
    "\n",
    "# Apply Majority Voting across models (Existing Code)\n",
    "majority_preds = mode(pred_stack, axis=0, keepdims=False).mode\n",
    "print(\"majority_preds:\", majority_preds.shape)\n",
    "\n",
    "# Max Rule: Choose the class with the highest probability across all models\n",
    "# max_preds = np.argmax(np.max(np.stack([res_densenet, res_inception, res_xception], axis=0), axis=0), axis=2)\n",
    "# print(\"max_preds:\", max_preds.shape)\n",
    "# Max Rule: Choose the class with the highest probability across all models\n",
    "# Stack the probabilities of the models (axis 0: stacking along the models dimension)\n",
    "stacked_probs = np.stack([res_densenet, res_inception, res_xception], axis=0)  # Shape: (3, 1887, num_classes)\n",
    "\n",
    "# Find the model with the maximum probability for each sample\n",
    "max_preds = np.argmax(np.max(stacked_probs, axis=0), axis=1)  # Axis 0: max across models, axis 1: pick class\n",
    "print(\"max_preds:\", max_preds.shape)\n",
    "\n",
    "\n",
    "# Product Rule: Multiply probabilities across models, then pick the class with the highest product\n",
    "product_probs = res_densenet * res_inception * res_xception\n",
    "product_preds = np.argmax(product_probs, axis=1)\n",
    "print(\"product_preds:\", product_preds.shape)\n",
    "\n",
    "# Suppose you evaluated each model on a validation set\n",
    "val_acc_inception = 0.971\n",
    "val_acc_densenet = 0.990\n",
    "val_acc_xception = 0.979\n",
    "\n",
    "# Sum of accuracies\n",
    "total = val_acc_inception + val_acc_densenet + val_acc_xception\n",
    "\n",
    "# Normalize to get weights for Weighted Averaging\n",
    "weights = [\n",
    "    val_acc_inception / total,\n",
    "    val_acc_densenet / total,\n",
    "    val_acc_xception / total\n",
    "]\n",
    "weights\n",
    "\n",
    "# Weighted Averaging: Apply the same logic as before for weighted probabilities\n",
    "weighted_probs = (\n",
    "    res_inception * weights[0] +\n",
    "    res_densenet * weights[1] +\n",
    "    res_xception * weights[2]\n",
    ")\n",
    "weighted_preds = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "# Average Probability: Average the predictions across all models\n",
    "avg_probs = (res_densenet + res_inception + res_xception) / 3\n",
    "avg_preds = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "# Fuzzy Rank-Based Fusion\n",
    "# Rank predictions for each sample across the three models (lower rank = better prediction)\n",
    "ranks_densenet = np.argsort(-res_densenet, axis=1)\n",
    "ranks_inception = np.argsort(-res_inception, axis=1)\n",
    "ranks_xception = np.argsort(-res_xception, axis=1)\n",
    "\n",
    "# Fuzzy rank calculation: inverse of ranks as fuzzy weights\n",
    "fuzzy_weights_densenet = 1 / (ranks_densenet + 1)  # Add 1 to avoid division by 0\n",
    "fuzzy_weights_inception = 1 / (ranks_inception + 1)\n",
    "fuzzy_weights_xception = 1 / (ranks_xception + 1)\n",
    "\n",
    "# Normalize fuzzy weights (fuzzy logic step)\n",
    "fuzzy_weights_densenet /= np.sum(fuzzy_weights_densenet, axis=1, keepdims=True)\n",
    "fuzzy_weights_inception /= np.sum(fuzzy_weights_inception, axis=1, keepdims=True)\n",
    "fuzzy_weights_xception /= np.sum(fuzzy_weights_xception, axis=1, keepdims=True)\n",
    "\n",
    "# Fuzzy Rank-Based Fusion: Multiply probabilities by fuzzy weights\n",
    "fuzzy_fusion_probs = (\n",
    "    res_densenet * fuzzy_weights_densenet +\n",
    "    res_inception * fuzzy_weights_inception +\n",
    "    res_xception * fuzzy_weights_xception\n",
    ")\n",
    "fuzzy_rank_preds = np.argmax(fuzzy_fusion_probs, axis=1)\n",
    "print(\"fuzzy_rank_preds:\", fuzzy_rank_preds.shape)\n",
    "\n",
    "# Example of true labels (modify this based on your dataset)\n",
    "true_labels = np.array(np.argmax(test_labels, axis=1)).astype(int)\n",
    "\n",
    "# Compute accuracy scores\n",
    "results = {\n",
    "    \"Majority Voting\": accuracy_score(true_labels, majority_preds) * 100,\n",
    "    \"Max Rule\": accuracy_score(true_labels, max_preds) * 100,\n",
    "    \"Product Rule\": accuracy_score(true_labels, product_preds) * 100,\n",
    "    \"Simple Averaging\": accuracy_score(true_labels, avg_preds) * 100,\n",
    "    \"Weighted Averaging\": accuracy_score(true_labels, weighted_preds) * 100,\n",
    "    \"Fuzzy Rank-Based Fusion\": accuracy_score(true_labels, fuzzy_rank_preds) * 100\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for method, acc in results.items():\n",
    "    print(f\"{method}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2daeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Get predictions from base models on the full test set\n",
    "res_densenet = model_densenet.predict(test_imgs)\n",
    "res_inception = model_inception.predict(test_imgs)\n",
    "res_xception = model_xception.predict(test_imgs)\n",
    "\n",
    "# Stack class predictions from all models\n",
    "pred_stack = np.stack([np.argmax(res_densenet, axis=1),\n",
    "                       np.argmax(res_inception, axis=1),\n",
    "                       np.argmax(res_xception, axis=1)], axis=0)  # shape: (3, 1887)\n",
    "\n",
    "# Apply Majority Voting across models\n",
    "majority_preds = mode(pred_stack, axis=0, keepdims=False).mode\n",
    "print(\"majority_preds:\", majority_preds.shape)\n",
    "\n",
    "# Max Rule: Choose the class with the highest probability across all models\n",
    "stacked_probs = np.stack([res_densenet, res_inception, res_xception], axis=0)  # Shape: (3, 1887, num_classes)\n",
    "max_preds = np.argmax(np.max(stacked_probs, axis=0), axis=1)  # Max across models\n",
    "print(\"max_preds:\", max_preds.shape)\n",
    "\n",
    "# Product Rule: Multiply probabilities across models, then pick the class with the highest product\n",
    "product_probs = res_densenet * res_inception * res_xception\n",
    "product_preds = np.argmax(product_probs, axis=1)\n",
    "print(\"product_preds:\", product_preds.shape)\n",
    "\n",
    "# Weighted Averaging (Using model validation accuracies to compute weights)\n",
    "val_acc_inception = 0.971\n",
    "val_acc_densenet = 0.990\n",
    "val_acc_xception = 0.979\n",
    "total = val_acc_inception + val_acc_densenet + val_acc_xception\n",
    "weights = [\n",
    "    val_acc_inception / total,\n",
    "    val_acc_densenet / total,\n",
    "    val_acc_xception / total\n",
    "]\n",
    "weighted_probs = (\n",
    "    res_inception * weights[0] +\n",
    "    res_densenet * weights[1] +\n",
    "    res_xception * weights[2]\n",
    ")\n",
    "weighted_preds = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "# Average Probability: Average the predictions across all models\n",
    "avg_probs = (res_densenet + res_inception + res_xception) / 3\n",
    "avg_preds = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "# Fuzzy Rank-Based Fusion\n",
    "ranks_densenet = np.argsort(-res_densenet, axis=1)\n",
    "ranks_inception = np.argsort(-res_inception, axis=1)\n",
    "ranks_xception = np.argsort(-res_xception, axis=1)\n",
    "\n",
    "fuzzy_weights_densenet = 1 / (ranks_densenet + 1)\n",
    "fuzzy_weights_inception = 1 / (ranks_inception + 1)\n",
    "fuzzy_weights_xception = 1 / (ranks_xception + 1)\n",
    "\n",
    "fuzzy_weights_densenet /= np.sum(fuzzy_weights_densenet, axis=1, keepdims=True)\n",
    "fuzzy_weights_inception /= np.sum(fuzzy_weights_inception, axis=1, keepdims=True)\n",
    "fuzzy_weights_xception /= np.sum(fuzzy_weights_xception, axis=1, keepdims=True)\n",
    "\n",
    "fuzzy_fusion_probs = (\n",
    "    res_densenet * fuzzy_weights_densenet +\n",
    "    res_inception * fuzzy_weights_inception +\n",
    "    res_xception * fuzzy_weights_xception\n",
    ")\n",
    "fuzzy_rank_preds = np.argmax(fuzzy_fusion_probs, axis=1)\n",
    "print(\"fuzzy_rank_preds:\", fuzzy_rank_preds.shape)\n",
    "\n",
    "# True labels\n",
    "true_labels = np.array(np.argmax(test_labels, axis=1)).astype(int)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics = {\n",
    "    \"Majority Voting\": {\n",
    "        \"accuracy\": accuracy_score(true_labels, majority_preds) * 100,\n",
    "        \"precision\": precision_score(true_labels, majority_preds, average='weighted') * 100,\n",
    "        \"recall\": recall_score(true_labels, majority_preds, average='weighted') * 100,\n",
    "        \"f1_score\": f1_score(true_labels, majority_preds, average='weighted') * 100\n",
    "    },\n",
    "    \"Max Rule\": {\n",
    "        \"accuracy\": accuracy_score(true_labels, max_preds) * 100,\n",
    "        \"precision\": precision_score(true_labels, max_preds, average='weighted') * 100,\n",
    "        \"recall\": recall_score(true_labels, max_preds, average='weighted') * 100,\n",
    "        \"f1_score\": f1_score(true_labels, max_preds, average='weighted') * 100\n",
    "    },\n",
    "    \"Product Rule\": {\n",
    "        \"accuracy\": accuracy_score(true_labels, product_preds) * 100,\n",
    "        \"precision\": precision_score(true_labels, product_preds, average='weighted') * 100,\n",
    "        \"recall\": recall_score(true_labels, product_preds, average='weighted') * 100,\n",
    "        \"f1_score\": f1_score(true_labels, product_preds, average='weighted') * 100\n",
    "    },\n",
    "    \"Simple Averaging\": {\n",
    "        \"accuracy\": accuracy_score(true_labels, avg_preds) * 100,\n",
    "        \"precision\": precision_score(true_labels, avg_preds, average='weighted') * 100,\n",
    "        \"recall\": recall_score(true_labels, avg_preds, average='weighted') * 100,\n",
    "        \"f1_score\": f1_score(true_labels, avg_preds, average='weighted') * 100\n",
    "    },\n",
    "    \"Weighted Averaging\": {\n",
    "        \"accuracy\": accuracy_score(true_labels, weighted_preds) * 100,\n",
    "        \"precision\": precision_score(true_labels, weighted_preds, average='weighted') * 100,\n",
    "        \"recall\": recall_score(true_labels, weighted_preds, average='weighted') * 100,\n",
    "        \"f1_score\": f1_score(true_labels, weighted_preds, average='weighted') * 100\n",
    "    },\n",
    "    \"Fuzzy Rank-Based Fusion\": {\n",
    "        \"accuracy\": accuracy_score(true_labels, fuzzy_rank_preds) * 100,\n",
    "        \"precision\": precision_score(true_labels, fuzzy_rank_preds, average='weighted') * 100,\n",
    "        \"recall\": recall_score(true_labels, fuzzy_rank_preds, average='weighted') * 100,\n",
    "        \"f1_score\": f1_score(true_labels, fuzzy_rank_preds, average='weighted') * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for method, scores in metrics.items():\n",
    "    print(f\"{method}:\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"  {metric}: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9cd15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf475a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
